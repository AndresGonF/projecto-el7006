{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "project_path = os.path.abspath('..')\n",
    "sys.path.insert(1, project_path)\n",
    "\n",
    "import numpy as np\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn\n",
    "# seaborn.set_context(context=\"talk\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.dataset import lc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N, n_dim, n_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = torch.nn.BatchNorm1d(layer.size)\n",
    "        self.linear = nn.Linear(n_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = torch.nn.BatchNorm1d(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x)[0])\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class periodicTransformer:\n",
    "    def __init__(self, n_classes, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "        self.model = self.make_model(n_classes, N, d_model, d_ff, h, dropout)\n",
    "\n",
    "    def make_model(self, n_classes, N, d_model, d_ff, h, dropout):\n",
    "        \"Helper: Construct a model from hyperparameters.\"\n",
    "        c = copy.deepcopy\n",
    "        attn = torch.nn.MultiheadAttention(d_model, h)\n",
    "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        position = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = EncoderLayer(d_model, c(attn), c(ff), dropout)\n",
    "        model = Encoder(encoder_layer, N, d_model, n_classes)\n",
    "        \n",
    "        # This was important from their code. \n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "        return model\n",
    "\n",
    "    def loss_function(self, data, label):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        return criterion(data, label)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['mag'], batch['label']\n",
    "        output = self.model(x)\n",
    "        loss = self.loss_function(output, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['mag'], batch['label']\n",
    "        output = self.model(x)\n",
    "        loss = self.loss_function(output, y)\n",
    "        acc = self.evaluate(output, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    def fit(self, train_loader, val_loader, n_epochs):\n",
    "        optimizer = self.configure_optimizers()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for idx, batch in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                loss = self.training_step(batch, idx)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            for idx, batch in enumerate(val_loader):\n",
    "                val_loss, val_acc = self.validation_step(batch, idx)\n",
    "            print(f'Epoch: {epoch} - Train loss: {loss} - Val loss: {val_loss} - Val acc: {val_acc}')\n",
    "        self.model = self.model\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            _, test_acc = self.validation_step(batch, idx)\n",
    "            print(f'Test acc: {test_acc}')\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        prediction = x.argmax(dim=1)\n",
    "        accuracy = (prediction == y).sum() / y.shape[0]\n",
    "        return accuracy        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lc_dataset()\n",
    "train_data.add_curves('sinmix', N=1000, seq_len=200, min_period=0.2, max_period=2, label=0)\n",
    "train_data.add_curves('square', N=1000, seq_len=200, min_period=0.2, max_period=2, label=1)\n",
    "\n",
    "val_data = lc_dataset(seed=127)\n",
    "val_data.add_curves('sinmix', N=500, seq_len=200, min_period=0.2, max_period=2, label=0)\n",
    "val_data.add_curves('square', N=500, seq_len=200, min_period=0.2, max_period=2, label=1)\n",
    "\n",
    "test_data = lc_dataset(seed=20)\n",
    "test_data.add_curves('sinmix', N=200, seq_len=200, min_period=0.1, max_period=1.8, label=0)\n",
    "test_data.add_curves('square', N=200, seq_len=200, min_period=0.1, max_period=1.8, label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                            batch_size=batch_size,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=16,\n",
    "                            shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_data,\n",
    "                        batch_size=batch_size,\n",
    "                        pin_memory=True,\n",
    "                        num_workers=16,\n",
    "                        shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                        batch_size=batch_size,\n",
    "                        pin_memory=True,\n",
    "                        num_workers=16,\n",
    "                        shuffle=True)                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_769/4150909005.py:21: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "tmp_model = periodicTransformer(n_classes=2, N=6, d_model=200, d_ff=2048, h=8, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Train loss: 0.0716586634516716 - Val loss: 0.1810898780822754 - Val acc: 0.925000011920929\n",
      "Epoch: 1 - Train loss: 0.14035557210445404 - Val loss: 0.11143292486667633 - Val acc: 0.925000011920929\n",
      "Epoch: 2 - Train loss: 0.049803175032138824 - Val loss: 0.07688991725444794 - Val acc: 0.9750000238418579\n",
      "Epoch: 3 - Train loss: 0.017067434266209602 - Val loss: 0.01705162599682808 - Val acc: 1.0\n",
      "Epoch: 4 - Train loss: 0.038137711584568024 - Val loss: 0.061437685042619705 - Val acc: 0.9750000238418579\n",
      "Epoch: 5 - Train loss: 0.18542662262916565 - Val loss: 0.019038742408156395 - Val acc: 1.0\n",
      "Epoch: 6 - Train loss: 0.014372097328305244 - Val loss: 0.04736971855163574 - Val acc: 0.9750000238418579\n",
      "Epoch: 7 - Train loss: 0.00014955521328374743 - Val loss: 0.15221865475177765 - Val acc: 0.925000011920929\n",
      "Epoch: 8 - Train loss: 0.06718696653842926 - Val loss: 0.022008059546351433 - Val acc: 1.0\n",
      "Epoch: 9 - Train loss: 0.015033786185085773 - Val loss: 0.01084776408970356 - Val acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "tmp_model.fit(train_loader, val_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.9375\n",
      "Test acc: 0.984375\n",
      "Test acc: 0.953125\n",
      "Test acc: 0.890625\n",
      "Test acc: 0.953125\n",
      "Test acc: 0.96875\n",
      "Test acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "test_data = lc_dataset(seed=20)\n",
    "test_data.add_curves('sinmix', N=200, seq_len=200, min_period=0.5, max_period=6, label=0)\n",
    "test_data.add_curves('square', N=200, seq_len=200, min_period=2, max_period=8, label=1)\n",
    "\n",
    "tmp_model.test(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "el7006",
   "language": "python",
   "name": "el7006"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
