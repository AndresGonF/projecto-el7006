{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agonzalez/miniconda3/envs/el7006/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/agonzalez/miniconda3/envs/el7006/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "project_path = os.path.abspath('..')\n",
    "sys.path.insert(1, project_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "# import seaborn\n",
    "# seaborn.set_context(context=\"talk\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math, copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.data.dataset import lc_dataset\n",
    "from src.models.model import periodicTransformer\n",
    "from src.visualization.plots import plot_periodic\n",
    "from src.data.curve_generator import random_periodic_sin_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len= 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        print(position.shape)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        print(div_term.shape)\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        print(self.pe[:x.size(0)].shape)\n",
    "        print(x.shape)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        print(x.shape)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add + norm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dropout=0.1, d_model=240, d_ff=128, h=8):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.mod = torch.nn.Linear(1, d_model)\n",
    "        self.linear1 = torch.nn.Linear(d_model, d_model)\n",
    "        self.att = torch.nn.MultiheadAttention(d_model, h)\n",
    "\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = x.float()\n",
    "        x = self.sublayer[0](x, lambda x: self.att(x, x, x)[0])\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFilm(nn.Module):\n",
    "    def __init__(self, n_harmonics=7, embedding_size=64, T_max=1000.0, input_size = 1):\n",
    "        super(TimeFilm, self).__init__()\n",
    "\n",
    "        self.a = nn.parameter.Parameter(\n",
    "            torch.rand(n_harmonics, embedding_size), requires_grad=True)\n",
    "        self.b = nn.parameter.Parameter(\n",
    "            torch.rand(n_harmonics, embedding_size), requires_grad=True)\n",
    "        self.w = nn.parameter.Parameter(\n",
    "            torch.rand(n_harmonics, embedding_size), requires_grad=True)\n",
    "        self.v = nn.parameter.Parameter(\n",
    "            torch.rand(n_harmonics, embedding_size),  requires_grad=True)\n",
    "\n",
    "        self.linear_proj = nn.Sequential(nn.Linear(in_features= input_size, out_features=embedding_size, bias=False),\n",
    "                                         nn.LeakyReLU(0.1))\n",
    "\n",
    "        self.linear_proj_ = nn.Sequential(nn.Linear(in_features=embedding_size, out_features=embedding_size, bias=False),\n",
    "                                          nn.LeakyReLU(0.1))\n",
    "        self.n_ = nn.parameter.Parameter(\n",
    "            torch.linspace(1, n_harmonics+1, steps=n_harmonics) / T_max, requires_grad=False)\n",
    "\n",
    "    def harmonics(self, t):\n",
    "        \"\"\" t [n_batch, length sequence, 1, n_harmonics]\"\"\"\n",
    "\n",
    "        return t[:, :, :, None]*2*np.pi*self.n_\n",
    "\n",
    "    def fourier_coefs(self, t):\n",
    "\n",
    "        t_harmonics = self.harmonics(t)\n",
    "\n",
    "        gama_ = torch.tanh(torch.matmul(torch.sin(t_harmonics), self.a) + \\\n",
    "            torch.matmul(torch.cos(t_harmonics), self.b))\n",
    "\n",
    "        beta_ = torch.matmul(torch.sin(t_harmonics), self.v) + \\\n",
    "            torch.matmul(torch.cos(t_harmonics), self.w)\n",
    "\n",
    "        return gama_, beta_\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\" t must be of size [n_batch, length sequence]\"\"\"\n",
    "        print(t.dtype)\n",
    "\n",
    "        gama_, beta_ = self.fourier_coefs(t)\n",
    "\n",
    "        # self.linear_proj_(self.linear_proj(x[:, :, None])*torch.tanh(torch.squeeze(gama_)) + torch.squeeze(beta_))\n",
    "        return self.linear_proj_(self.linear_proj(x)*torch.squeeze(gama_) + torch.squeeze(beta_))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingSousa(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=200, max_time=1000.0, max_len= 5000):\n",
    "        super(PositionalEncodingSousa, self).__init__()\n",
    "        self.div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(max_time) / d_model))\n",
    "        self.pe = torch.zeros(max_len, 1000, d_model)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        argument = t * self.div_term\n",
    "        self.pe[:, :, 0::2] = torch.sin(argument)\n",
    "        self.pe[:, :, 1::2] = torch.cos(argument)\n",
    "        self.register_buffer('pe', self.pe)        \n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class periodicTransformer(nn.Module):\n",
    "    def __init__(self, n_classes=5, d_model=200, d_ff=128, h=8, N=4, time='discrete'):\n",
    "        super().__init__()\n",
    "        self.time = time\n",
    "        self.pos_enc_discrete = PositionalEncoding(d_model)\n",
    "        self.pos_enc_continuous = TimeFilm(embedding_size=d_model)\n",
    "        self.enc_blocks = clones(EncoderBlock(d_model=d_model, d_ff=d_ff, h=h), N)\n",
    "        self.proj = nn.Linear(d_model, n_classes)\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if self.time == 'continuous':\n",
    "            x = self.pos_enc_continuous(x, t)\n",
    "        else:\n",
    "            x = self.pos_enc_discrete(x)\n",
    "        for enc in self.enc_blocks:\n",
    "            x = enc(x)\n",
    "        x = self.proj(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28412/4208361457.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "d_model = 200\n",
    "\n",
    "temp = periodicTransformer(d_model=d_model).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "torch.Size([2, 60, 1])\n",
      "torch.Size([2, 60, 200])\n"
     ]
    }
   ],
   "source": [
    "output = temp(batch['mag'].unsqueeze(dim=2), batch['mjd'].unsqueeze(dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,-1] == output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 200, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lc_dataset()\n",
    "data.add_curves('sinmix', N=800, seq_len=60, min_period=0.5, max_period=2, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "data_loader = DataLoader(data,\n",
    "                        batch_size=batch_size,\n",
    "                        pin_memory=True,\n",
    "                        num_workers=16,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(data_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "el7006",
   "language": "python",
   "name": "el7006"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
